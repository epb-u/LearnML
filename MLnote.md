# 1 机器学习概论

机器学习是人工智能的一个领域

## 1.1机器学习定义

Arthur Samuel(1959):在没有明确设置的情况下使计算机具有学习能力的研究领域

Tom Mitchell(1998):计算机程序从经验E中学习，解决某任务T，进行某一性能度量P，通过P测定在T上的表现因经验E而提高

机器学习的分类：1.监督学习 	2.无监督学习

## 1.2监督学习

​	简单定义：训练集中包含”正确答案“

监督学习的典型任务：1.回归 2. 分类

回归：通过一组称为预测器的特征(里程，使用年限，品牌等)来预测一个目标值（如汽车的售价）

分类：通过对数据集分类，预测目标的类别

## 1.3无监督学习

​	简单定义：训练集中不包含“答案”

无监督学习的典型人物：1.聚类

# 2 单变量线性回归

## 2.1 模型描述

![traning example](pic/traning example.png)

m = 训练集的大小

x = 输入变量/特征

y = 输出/目标变量

(x,y) = 一个训练样本

($x^i,y^i$) = 第i个训练样本

![hypothesis](pic/hypothesis.png)

​	监督学习算法以一个训练集为输入，输出一个函数h,这个函数把房子的大小x作为输入变量，输出相应房子的预测售价y

​	如何表达h? 一种可能的表达方式为：$h_\theta \left( x \right)=\theta_{0} + \theta_{1}x$，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。其中$\theta_{i}$称为模型参数

## 2.2 代价函数

​	如何选择合适的模型参数？比如$\theta_{0}$选1.5，$\theta_{1}$选0怎么样？

​	我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是**建模误差**（**modeling error**）。

![](./pic/modeling error.png)

​	因此我们的目标就是选出合适的模型参数，让模型误差的平方和能够最小，即代价函数 $J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$最小。

​	注意到$J\left( \theta_0, \theta_1 \right)$是一个二元函数，绘制一个等高线，三个坐标分别为$\theta_{0}$和$\theta_{1}$ 和$J(\theta_{0}, \theta_{1})$：

![](./pic/bivar function.png)

​	可以看到该三维空间中存在一个点$\left( \theta_0, \theta_1 \right)$使得$J$最小。

​	代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。

## 	2.3梯度下降算法

​	梯度下降算法是一个用来求函数最小值的算法，算法思想是：先随机设定一组参数值$\left( {\theta_{0}},{\theta_{1}},......,{\theta_{n}} \right)$,计算代价函数，然后我们寻找一下个能让代价函数**下降的最多**的参数值，重复这个过程就能找到一个局部最小值。

​	如果将代价函数$J$看作一座山，那么这个过程就类似于随机选择山上的一个点，然后从那个点360看一圈，找到下山最快的那个方向，走到下一个点，重复这个过程直到走到局部最低点。根据最开始选择的点的位置，会走到不同的局部最低点，在所有局部最低点中，最低的那个点就是“山底”。

​	批量梯度下降（**batch gradient descent**）算法的公式为：

![](./pic/batch gradient descent.png)

​	注意：这里必须是同时计算temp0和temp1,计算完temp0后直接赋值给$\theta_0$会导致计算tmp1时出现错误。$\alpha$：学习速率

**解析递归下降算法：**

​	![](./pic/explain bgd.png)

​	先假设一个更简单的代价函数$J(\theta)$，同时学习速率为正数

​	当初始位置在最小值点右侧时，导数为正数，因此$\theta_0$会变小。

​	同理，在左侧时，导数为负，$\theta_0$会变大。

​	因此更新方程总是能将$\theta_0$往最小值点“移动”，随着算法不断进行，到达最小值点时，由于导数为0，$\theta_0$就会停止"移动"。

​	当$\theta$逐渐接近最小值点时，斜率会逐渐降为0，因此导数项会逐渐变小

**对于学习速率$\alpha$的分析：**

​	如果$\alpha$太小，梯度下降速度会变小，$\theta$到达最小值点的时间就会变长

​	如果$\alpha$太大，$\theta$就有可能会"跨过"最小值点，即无法收敛到最小值点

**梯度下降算法的问题**：根据初始点的不同，会得到不同的局部最小点

## 2.4 梯度下降算法在线性回归的应用

梯段下降算法和线性回归算法如下图：

![](./pic/GL.png)

$\frac{\partial }{\partial {{\theta }_{j}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{\partial }{\partial {{\theta }_{j}}}\frac{1}{2m}{{\sum\limits_{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}^{2}}$

$j=0$  时：$\frac{\partial }{\partial {{\theta }_{0}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{1}{m}{{\sum\limits_{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}}$

$j=1$  时：$\frac{\partial }{\partial {{\theta }_{1}}}J({{\theta }_{0}},{{\theta }_{1}})=\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$

则算法改写成：

**Repeat {**

​                ${\theta_{0}}:={\theta_{0}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{ \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}$

​                ${\theta_{1}}:={\theta_{1}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$

​               **}**

特点：梯度下降算法在线性回归问题中，不会出现局部最优解的问题，这是因为代价函数$J$是个凸函数，只有全局最优解

# 3 线性代数

略

# 4多变量线性回归

​	在第三章中讨论了单变量的回归模型，现在讨论具有更多变量的模型，模型中的特征为$\left( {x_{1}},{x_{2}},...,{x_{n}} \right)$。

​	![](pic/4-1.png)

${x^{\left( i \right)}}$代表第 $i$ 个训练实例，是特征矩阵中的第$i$行，是一个**向量**（**vector**）。

比方说，上图的

${x}^{(2)}\text{=}\begin{bmatrix} 1416\\\ 3\\\ 2\\\ 40 \end{bmatrix}$，

${x}_{j}^{\left( i \right)}$代表特征矩阵中第 $i$ 行的第 $j$ 个特征，也就是第 $i$ 个训练实例的第 $j$ 个特征。

支持多变量的假设 $h$ 表示为：$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$，

为了使得公式能够简化一些，引入$x_{0}=1$，则公式转化为：$h_{\theta} \left( x \right)={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$

此时模型中的参数是一个$n+1$维的向量，任何一个训练实例也都是$n+1$维的向量，特征矩阵$X$的维度是 $m*(n+1)$。 因此公式可以简化为：$h_{\theta} \left( x \right)={\theta^{T}}X$，其中上标$T$代表矩阵转置。
